{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04f32999-31fc-4cb0-b175-d4fef2593d3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Schemat danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92ea5bb9-4e0c-439f-9905-a486c0316b4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Pobieramy plik csv - movies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5513b5e-dd7f-4908-8888-1e84f85345e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/Files/movies.csv</td><td>movies.csv</td><td>50657522</td><td>1741713485000</td></tr><tr><td>dbfs:/FileStore/tables/Files/new_movies.csv/</td><td>new_movies.csv/</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/Files/movies.csv",
         "movies.csv",
         50657522,
         1741713485000
        ],
        [
         "dbfs:/FileStore/tables/Files/new_movies.csv/",
         "new_movies.csv/",
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- imdb_title_id: string (nullable = true)\n |-- title: string (nullable = true)\n |-- original_title: string (nullable = true)\n |-- year: string (nullable = true)\n |-- date_published: string (nullable = true)\n |-- genre: string (nullable = true)\n |-- duration: string (nullable = true)\n |-- country: string (nullable = true)\n |-- language: string (nullable = true)\n |-- director: string (nullable = true)\n |-- writer: string (nullable = true)\n |-- production_company: string (nullable = true)\n |-- actors: string (nullable = true)\n |-- description: string (nullable = true)\n |-- avg_vote: string (nullable = true)\n |-- votes: string (nullable = true)\n |-- budget: string (nullable = true)\n |-- usa_gross_income: string (nullable = true)\n |-- worlwide_gross_income: string (nullable = true)\n |-- metascore: string (nullable = true)\n |-- reviews_from_users: string (nullable = true)\n |-- reviews_from_critics: string (nullable = true)\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>imdb_title_id</th><th>title</th><th>original_title</th><th>year</th><th>date_published</th><th>genre</th><th>duration</th><th>country</th><th>language</th><th>director</th><th>writer</th><th>production_company</th><th>actors</th><th>description</th><th>avg_vote</th><th>votes</th><th>budget</th><th>usa_gross_income</th><th>worlwide_gross_income</th><th>metascore</th><th>reviews_from_users</th><th>reviews_from_critics</th></tr></thead><tbody><tr><td>tt0000009</td><td>Miss Jerry</td><td>Miss Jerry</td><td>1894</td><td>1894-10-09</td><td>Romance</td><td>45</td><td>USA</td><td>None</td><td>Alexander Black</td><td>Alexander Black</td><td>Alexander Black Photoplays</td><td>Blanche Bayliss, William Courtenay, Chauncey Depew</td><td>The adventures of a female reporter in the 1890s.</td><td>05.wrz</td><td>154</td><td>null</td><td>null</td><td>null</td><td>null</td><td>1.0</td><td>2.0</td></tr><tr><td>tt0000574</td><td>The Story of the Kelly Gang</td><td>The Story of the Kelly Gang</td><td>1906</td><td>26.12.1906</td><td>Biography, Crime, Drama</td><td>70</td><td>Australia</td><td>None</td><td>Charles Tait</td><td>Charles Tait</td><td>J. and N. Tait</td><td>Elizabeth Tait, John Tait, Norman Campbell, Bella Cola, Will Coyne, Sam Crewes, Jack Ennis, John Forde, Vera Linden, Mr. Marshall, Mr. McKenzie, Frank Mills, Ollie Wilson</td><td>True story of notorious Australian outlaw Ned Kelly (1855-80).</td><td>06.sty</td><td>589</td><td>$ 2250</td><td>null</td><td>null</td><td>null</td><td>7.0</td><td>7.0</td></tr><tr><td>tt0001892</td><td>Den sorte drøm</td><td>Den sorte drøm</td><td>1911</td><td>19.08.1911</td><td>Drama</td><td>53</td><td>Germany, Denmark</td><td>null</td><td>Urban Gad</td><td>Urban Gad, Gebhard Schätzler-Perasini</td><td>Fotorama</td><td>Asta Nielsen, Valdemar Psilander, Gunnar Helsengreen, Emil Albes, Hugo Flink, Mary Hagen</td><td>Two men of high rank are both wooing the beautiful and famous equestrian acrobat Stella. While Stella ignores the jeweler Hirsch, she accepts Count von Waldberg's offer to follow her home, ...</td><td>05.sie</td><td>188</td><td>null</td><td>null</td><td>null</td><td>null</td><td>5.0</td><td>2.0</td></tr><tr><td>tt0002101</td><td>Cleopatra</td><td>Cleopatra</td><td>1912</td><td>13.11.1912</td><td>Drama, History</td><td>100</td><td>USA</td><td>English</td><td>Charles L. Gaskill</td><td>Victorien Sardou</td><td>Helen Gardner Picture Players</td><td>Helen Gardner, Pearl Sindelar, Miss Fielding, Miss Robson, Helene Costello, Charles Sindelar, Mr. Howard, James R. Waite, Mr. Osborne, Harry Knowles, Mr. Paul, Mr. Brady, Mr. Corker</td><td>The fabled queen of Egypt's affair with Roman general Marc Antony is ultimately disastrous for both of them.</td><td>05.lut</td><td>446</td><td>$ 45000</td><td>null</td><td>null</td><td>null</td><td>25.0</td><td>3.0</td></tr><tr><td>tt0002130</td><td>L'Inferno</td><td>L'Inferno</td><td>1911</td><td>06.03.1911</td><td>Adventure, Drama, Fantasy</td><td>68</td><td>Italy</td><td>Italian</td><td>Francesco Bertolini, Adolfo Padovan</td><td>Dante Alighieri</td><td>Milano Film</td><td>Salvatore Papa, Arturo Pirovano, Giuseppe de Liguoro, Pier Delle Vigne, Augusto Milla, Attilio Motta, Emilise Beretta</td><td>Loosely adapted from Dante's Divine Comedy and inspired by the illustrations of Gustav Doré the original silent film has been restored and has a new score by Tangerine Dream.</td><td>7.0</td><td>2237</td><td>null</td><td>null</td><td>null</td><td>null</td><td>31.0</td><td>14.0</td></tr><tr><td>tt0002199</td><td>From the Manger to the Cross or, Jesus of Nazareth</td><td>From the Manger to the Cross or, Jesus of Nazareth</td><td>1912</td><td>1913</td><td>Biography, Drama</td><td>60</td><td>USA</td><td>English</td><td>Sidney Olcott</td><td>Gene Gauntier</td><td>Kalem Company</td><td>R. Henderson Bland, Percy Dyer, Gene Gauntier, Alice Hollister, Samuel Morgan, James D. Ainsley, Robert G. Vignola, George Kellog, J.P. McGowan</td><td>An account of the life of Jesus Christ, based on the books of the New Testament: After Jesus' birth is foretold to his parents, he is born in Bethlehem, and is visited by shepherds and wise...</td><td>05.lip</td><td>484</td><td>null</td><td>null</td><td>null</td><td>null</td><td>13.0</td><td>5.0</td></tr><tr><td>tt0002423</td><td>Madame DuBarry</td><td>Madame DuBarry</td><td>1919</td><td>26.11.1919</td><td>Biography, Drama, Romance</td><td>85</td><td>Germany</td><td>German</td><td>Ernst Lubitsch</td><td>Norbert Falk, Hanns Kräly</td><td>Projektions-AG Union (PAGU)</td><td>Pola Negri, Emil Jannings, Harry Liedtke, Eduard von Winterstein, Reinhold Schünzel, Else Berna, Fred Immler, Gustav Czimeg, Karl Platen, Bernhard Goetzke, Magnus Stifter, Paul Biensfeldt, Willy Kaiser-Heyl, Alexander Ekert, Robert Sortsch-Pla</td><td>The story of Madame DuBarry, the mistress of Louis XV of France, and her loves in the time of the French revolution.</td><td>06.sie</td><td>753</td><td>null</td><td>null</td><td>null</td><td>null</td><td>12.0</td><td>9.0</td></tr><tr><td>tt0002445</td><td>Quo Vadis?</td><td>Quo Vadis?</td><td>1913</td><td>01.03.1913</td><td>Drama, History</td><td>120</td><td>Italy</td><td>Italian</td><td>Enrico Guazzoni</td><td>Henryk Sienkiewicz, Enrico Guazzoni</td><td>Società Italiana Cines</td><td>Amleto Novelli, Gustavo Serena, Carlo Cattaneo, Amelia Cattaneo, Lea Giunchi, Bruto Castellani, Augusto Mastripietri, Cesare Moltini, Olga Brandini, Ignazio Lupi, Giovanni Gizzi, Lia Orlandini, Matilde Guillaume, Ida Carloni Talli, Giuseppe Gambardella</td><td>An epic Italian film Quo Vadis influenced many of the later movies.</td><td>06.lut</td><td>273</td><td>ITL 45000</td><td>null</td><td>null</td><td>null</td><td>7.0</td><td>5.0</td></tr><tr><td>tt0002452</td><td>Independenta Romaniei</td><td>Independenta Romaniei</td><td>1912</td><td>01.09.1912</td><td>History, War</td><td>120</td><td>Romania</td><td>null</td><td>Aristide Demetriade, Grigore Brezeanu</td><td>Aristide Demetriade, Petre Liciu</td><td>Societatea Filmului de Arta Leon Popescu</td><td>Aristide Demetriade, Constanta Demetriade, Constantin Nottara, Pepi Machauer, Aurel Athanasescu, Jeny Metaxa-Doro, Nicolae Soreanu, Vasile Toneanu, Aristita Romanescu, Elvire Popesco, M. Vîrgolici, C. Nedelcovici, Mihail Tancovici-Cosmin, Ion Dumitrescu, Gheorghe Meliseanu</td><td>The movie depicts the Romanian War of Independence (1877-1878).</td><td>06.lip</td><td>198</td><td>ROL 400000</td><td>null</td><td>null</td><td>null</td><td>4.0</td><td>1.0</td></tr><tr><td>tt0002461</td><td>Richard III</td><td>Richard III</td><td>1912</td><td>15.10.1912</td><td>Drama</td><td>55</td><td>France, USA</td><td>English</td><td>André Calmettes, James Keane</td><td>James Keane, William Shakespeare</td><td>Le Film d'Art</td><td>Robert Gemp, Frederick Warde, Albert Gardner, James Keane, George Moss, Howard Stuart, Virginia Rankin, Violet Stuart, Carey Lee, Carlotta De Felice</td><td>Richard of Gloucester uses manipulation and murder to gain the English throne.</td><td>05.maj</td><td>225</td><td>$ 30000</td><td>null</td><td>null</td><td>null</td><td>8.0</td><td>1.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "tt0000009",
         "Miss Jerry",
         "Miss Jerry",
         "1894",
         "1894-10-09",
         "Romance",
         "45",
         "USA",
         "None",
         "Alexander Black",
         "Alexander Black",
         "Alexander Black Photoplays",
         "Blanche Bayliss, William Courtenay, Chauncey Depew",
         "The adventures of a female reporter in the 1890s.",
         "05.wrz",
         "154",
         null,
         null,
         null,
         null,
         "1.0",
         "2.0"
        ],
        [
         "tt0000574",
         "The Story of the Kelly Gang",
         "The Story of the Kelly Gang",
         "1906",
         "26.12.1906",
         "Biography, Crime, Drama",
         "70",
         "Australia",
         "None",
         "Charles Tait",
         "Charles Tait",
         "J. and N. Tait",
         "Elizabeth Tait, John Tait, Norman Campbell, Bella Cola, Will Coyne, Sam Crewes, Jack Ennis, John Forde, Vera Linden, Mr. Marshall, Mr. McKenzie, Frank Mills, Ollie Wilson",
         "True story of notorious Australian outlaw Ned Kelly (1855-80).",
         "06.sty",
         "589",
         "$ 2250",
         null,
         null,
         null,
         "7.0",
         "7.0"
        ],
        [
         "tt0001892",
         "Den sorte drøm",
         "Den sorte drøm",
         "1911",
         "19.08.1911",
         "Drama",
         "53",
         "Germany, Denmark",
         null,
         "Urban Gad",
         "Urban Gad, Gebhard Schätzler-Perasini",
         "Fotorama",
         "Asta Nielsen, Valdemar Psilander, Gunnar Helsengreen, Emil Albes, Hugo Flink, Mary Hagen",
         "Two men of high rank are both wooing the beautiful and famous equestrian acrobat Stella. While Stella ignores the jeweler Hirsch, she accepts Count von Waldberg's offer to follow her home, ...",
         "05.sie",
         "188",
         null,
         null,
         null,
         null,
         "5.0",
         "2.0"
        ],
        [
         "tt0002101",
         "Cleopatra",
         "Cleopatra",
         "1912",
         "13.11.1912",
         "Drama, History",
         "100",
         "USA",
         "English",
         "Charles L. Gaskill",
         "Victorien Sardou",
         "Helen Gardner Picture Players",
         "Helen Gardner, Pearl Sindelar, Miss Fielding, Miss Robson, Helene Costello, Charles Sindelar, Mr. Howard, James R. Waite, Mr. Osborne, Harry Knowles, Mr. Paul, Mr. Brady, Mr. Corker",
         "The fabled queen of Egypt's affair with Roman general Marc Antony is ultimately disastrous for both of them.",
         "05.lut",
         "446",
         "$ 45000",
         null,
         null,
         null,
         "25.0",
         "3.0"
        ],
        [
         "tt0002130",
         "L'Inferno",
         "L'Inferno",
         "1911",
         "06.03.1911",
         "Adventure, Drama, Fantasy",
         "68",
         "Italy",
         "Italian",
         "Francesco Bertolini, Adolfo Padovan",
         "Dante Alighieri",
         "Milano Film",
         "Salvatore Papa, Arturo Pirovano, Giuseppe de Liguoro, Pier Delle Vigne, Augusto Milla, Attilio Motta, Emilise Beretta",
         "Loosely adapted from Dante's Divine Comedy and inspired by the illustrations of Gustav Doré the original silent film has been restored and has a new score by Tangerine Dream.",
         "7.0",
         "2237",
         null,
         null,
         null,
         null,
         "31.0",
         "14.0"
        ],
        [
         "tt0002199",
         "From the Manger to the Cross or, Jesus of Nazareth",
         "From the Manger to the Cross or, Jesus of Nazareth",
         "1912",
         "1913",
         "Biography, Drama",
         "60",
         "USA",
         "English",
         "Sidney Olcott",
         "Gene Gauntier",
         "Kalem Company",
         "R. Henderson Bland, Percy Dyer, Gene Gauntier, Alice Hollister, Samuel Morgan, James D. Ainsley, Robert G. Vignola, George Kellog, J.P. McGowan",
         "An account of the life of Jesus Christ, based on the books of the New Testament: After Jesus' birth is foretold to his parents, he is born in Bethlehem, and is visited by shepherds and wise...",
         "05.lip",
         "484",
         null,
         null,
         null,
         null,
         "13.0",
         "5.0"
        ],
        [
         "tt0002423",
         "Madame DuBarry",
         "Madame DuBarry",
         "1919",
         "26.11.1919",
         "Biography, Drama, Romance",
         "85",
         "Germany",
         "German",
         "Ernst Lubitsch",
         "Norbert Falk, Hanns Kräly",
         "Projektions-AG Union (PAGU)",
         "Pola Negri, Emil Jannings, Harry Liedtke, Eduard von Winterstein, Reinhold Schünzel, Else Berna, Fred Immler, Gustav Czimeg, Karl Platen, Bernhard Goetzke, Magnus Stifter, Paul Biensfeldt, Willy Kaiser-Heyl, Alexander Ekert, Robert Sortsch-Pla",
         "The story of Madame DuBarry, the mistress of Louis XV of France, and her loves in the time of the French revolution.",
         "06.sie",
         "753",
         null,
         null,
         null,
         null,
         "12.0",
         "9.0"
        ],
        [
         "tt0002445",
         "Quo Vadis?",
         "Quo Vadis?",
         "1913",
         "01.03.1913",
         "Drama, History",
         "120",
         "Italy",
         "Italian",
         "Enrico Guazzoni",
         "Henryk Sienkiewicz, Enrico Guazzoni",
         "Società Italiana Cines",
         "Amleto Novelli, Gustavo Serena, Carlo Cattaneo, Amelia Cattaneo, Lea Giunchi, Bruto Castellani, Augusto Mastripietri, Cesare Moltini, Olga Brandini, Ignazio Lupi, Giovanni Gizzi, Lia Orlandini, Matilde Guillaume, Ida Carloni Talli, Giuseppe Gambardella",
         "An epic Italian film Quo Vadis influenced many of the later movies.",
         "06.lut",
         "273",
         "ITL 45000",
         null,
         null,
         null,
         "7.0",
         "5.0"
        ],
        [
         "tt0002452",
         "Independenta Romaniei",
         "Independenta Romaniei",
         "1912",
         "01.09.1912",
         "History, War",
         "120",
         "Romania",
         null,
         "Aristide Demetriade, Grigore Brezeanu",
         "Aristide Demetriade, Petre Liciu",
         "Societatea Filmului de Arta Leon Popescu",
         "Aristide Demetriade, Constanta Demetriade, Constantin Nottara, Pepi Machauer, Aurel Athanasescu, Jeny Metaxa-Doro, Nicolae Soreanu, Vasile Toneanu, Aristita Romanescu, Elvire Popesco, M. Vîrgolici, C. Nedelcovici, Mihail Tancovici-Cosmin, Ion Dumitrescu, Gheorghe Meliseanu",
         "The movie depicts the Romanian War of Independence (1877-1878).",
         "06.lip",
         "198",
         "ROL 400000",
         null,
         null,
         null,
         "4.0",
         "1.0"
        ],
        [
         "tt0002461",
         "Richard III",
         "Richard III",
         "1912",
         "15.10.1912",
         "Drama",
         "55",
         "France, USA",
         "English",
         "André Calmettes, James Keane",
         "James Keane, William Shakespeare",
         "Le Film d'Art",
         "Robert Gemp, Frederick Warde, Albert Gardner, James Keane, George Moss, Howard Stuart, Virginia Rankin, Violet Stuart, Carey Lee, Carlotta De Felice",
         "Richard of Gloucester uses manipulation and murder to gain the English throne.",
         "05.maj",
         "225",
         "$ 30000",
         null,
         null,
         null,
         "8.0",
         "1.0"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "imdb_title_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "title",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "original_title",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "year",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "date_published",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "genre",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "duration",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "language",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "director",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "writer",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "production_company",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "actors",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "description",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "avg_vote",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "votes",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "budget",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "usa_gross_income",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "worlwide_gross_income",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "metascore",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "reviews_from_users",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "reviews_from_critics",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MOVIES\n",
    "\n",
    "# link do pliku\n",
    "moviesUrl = \"https://raw.githubusercontent.com/cegladanych/azure_bi_data/main/IMDB_movies/movies.csv\"\n",
    "filePath = \"/FileStore/tables/Files/\"\n",
    "dbutils.fs.mkdirs(filePath)\n",
    "moviesFile = \"movies.csv\"\n",
    "dbfsdestination = \"dbfs:/FileStore/tables/Files/\"\n",
    "\n",
    "#fileFullPath = \"dbfs:/FileStore/tables/Files/movies.csv\"\n",
    "fileFullPath  = dbfsdestination + moviesFile \n",
    "\n",
    "# wczytujemy plik\n",
    "moviesDf = spark.read.format(\"csv\") \\\n",
    "            .option(\"header\",\"true\") \\\n",
    "            .load(fileFullPath)\n",
    "\n",
    "display(dbutils.fs.ls(\"dbfs:/FileStore/tables/Files/\")) # zawawrtosc katalogu\n",
    "moviesDf.printSchema() # table i typy wartosci w dataframe movies\n",
    "display(moviesDf.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ee79f93-659b-48ab-86c3-abc92baecd59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Definujemy schemat i wczytujemu plik z użyciem schematu\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75a1769c-907c-445e-bbd7-47488dd18104",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bc48497-9b8d-4a8e-9587-f9cdc46cf0d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>tytuł</th><th>oryginalny tytuł</th><th>rok</th></tr></thead><tbody><tr><td>tt0000009</td><td>Miss Jerry</td><td>Miss Jerry</td><td>1894</td></tr><tr><td>tt0000574</td><td>The Story of the Kelly Gang</td><td>The Story of the Kelly Gang</td><td>1906</td></tr><tr><td>tt0001892</td><td>Den sorte drøm</td><td>Den sorte drøm</td><td>1911</td></tr><tr><td>tt0002101</td><td>Cleopatra</td><td>Cleopatra</td><td>1912</td></tr><tr><td>tt0002130</td><td>L'Inferno</td><td>L'Inferno</td><td>1911</td></tr><tr><td>tt0002199</td><td>From the Manger to the Cross or, Jesus of Nazareth</td><td>From the Manger to the Cross or, Jesus of Nazareth</td><td>1912</td></tr><tr><td>tt0002423</td><td>Madame DuBarry</td><td>Madame DuBarry</td><td>1919</td></tr><tr><td>tt0002445</td><td>Quo Vadis?</td><td>Quo Vadis?</td><td>1913</td></tr><tr><td>tt0002452</td><td>Independenta Romaniei</td><td>Independenta Romaniei</td><td>1912</td></tr><tr><td>tt0002461</td><td>Richard III</td><td>Richard III</td><td>1912</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "tt0000009",
         "Miss Jerry",
         "Miss Jerry",
         "1894"
        ],
        [
         "tt0000574",
         "The Story of the Kelly Gang",
         "The Story of the Kelly Gang",
         "1906"
        ],
        [
         "tt0001892",
         "Den sorte drøm",
         "Den sorte drøm",
         "1911"
        ],
        [
         "tt0002101",
         "Cleopatra",
         "Cleopatra",
         "1912"
        ],
        [
         "tt0002130",
         "L'Inferno",
         "L'Inferno",
         "1911"
        ],
        [
         "tt0002199",
         "From the Manger to the Cross or, Jesus of Nazareth",
         "From the Manger to the Cross or, Jesus of Nazareth",
         "1912"
        ],
        [
         "tt0002423",
         "Madame DuBarry",
         "Madame DuBarry",
         "1919"
        ],
        [
         "tt0002445",
         "Quo Vadis?",
         "Quo Vadis?",
         "1913"
        ],
        [
         "tt0002452",
         "Independenta Romaniei",
         "Independenta Romaniei",
         "1912"
        ],
        [
         "tt0002461",
         "Richard III",
         "Richard III",
         "1912"
        ]
       ],
       "datasetInfos": [
        {
         "name": "moviesS",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "tytuł",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "oryginalny tytuł",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "rok",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "org.apache.spark.sql.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tytuł",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "oryginalny tytuł",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "rok",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val schemat = StructType(Array( \n",
    "    StructField(\"id\", StringType, false),\n",
    "    StructField(\"tytuł\", StringType, false),\n",
    "    StructField(\"oryginalny tytuł\", StringType, false),\n",
    "    StructField(\"rok\", StringType, false)))\n",
    "\n",
    "val moviesS = spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    // .option(\"inferSchema\", \"true\") // schemat\n",
    "    .schema(schemat)\n",
    "    .load(\"dbfs:/FileStore/tables/Files/movies.csv\")\n",
    "\n",
    "display(moviesS.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9832e33f-2a17-4a36-b5cf-9bb6f8aaa311",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "960ee926-6153-474d-a00a-ca490472da14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>tytuł</th><th>oryginalny tytuł</th><th>rok</th></tr></thead><tbody><tr><td>tt0000009</td><td>Miss Jerry</td><td>Miss Jerry</td><td>1894</td></tr><tr><td>tt0000574</td><td>The Story of the Kelly Gang</td><td>The Story of the Kelly Gang</td><td>1906</td></tr><tr><td>tt0001892</td><td>Den sorte drøm</td><td>Den sorte drøm</td><td>1911</td></tr><tr><td>tt0002101</td><td>Cleopatra</td><td>Cleopatra</td><td>1912</td></tr><tr><td>tt0002130</td><td>L'Inferno</td><td>L'Inferno</td><td>1911</td></tr><tr><td>tt0002199</td><td>From the Manger to the Cross or, Jesus of Nazareth</td><td>From the Manger to the Cross or, Jesus of Nazareth</td><td>1912</td></tr><tr><td>tt0002423</td><td>Madame DuBarry</td><td>Madame DuBarry</td><td>1919</td></tr><tr><td>tt0002445</td><td>Quo Vadis?</td><td>Quo Vadis?</td><td>1913</td></tr><tr><td>tt0002452</td><td>Independenta Romaniei</td><td>Independenta Romaniei</td><td>1912</td></tr><tr><td>tt0002461</td><td>Richard III</td><td>Richard III</td><td>1912</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "tt0000009",
         "Miss Jerry",
         "Miss Jerry",
         "1894"
        ],
        [
         "tt0000574",
         "The Story of the Kelly Gang",
         "The Story of the Kelly Gang",
         "1906"
        ],
        [
         "tt0001892",
         "Den sorte drøm",
         "Den sorte drøm",
         "1911"
        ],
        [
         "tt0002101",
         "Cleopatra",
         "Cleopatra",
         "1912"
        ],
        [
         "tt0002130",
         "L'Inferno",
         "L'Inferno",
         "1911"
        ],
        [
         "tt0002199",
         "From the Manger to the Cross or, Jesus of Nazareth",
         "From the Manger to the Cross or, Jesus of Nazareth",
         "1912"
        ],
        [
         "tt0002423",
         "Madame DuBarry",
         "Madame DuBarry",
         "1919"
        ],
        [
         "tt0002445",
         "Quo Vadis?",
         "Quo Vadis?",
         "1913"
        ],
        [
         "tt0002452",
         "Independenta Romaniei",
         "Independenta Romaniei",
         "1912"
        ],
        [
         "tt0002461",
         "Richard III",
         "Richard III",
         "1912"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tytuł",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "oryginalny tytuł",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "rok",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schemat = StructType([\n",
    "    StructField(\"id\", StringType(), False),\n",
    "    StructField(\"tytuł\", StringType(), False),\n",
    "    StructField(\"oryginalny tytuł\", StringType(), False),\n",
    "    StructField(\"rok\", StringType(), False)\n",
    "])\n",
    "\n",
    "# moviesDf = spark.read.format(\"csv\") \\\n",
    "#             .option(\"header\",\"true\") \\\n",
    "#             .load(\"dbfs:/FileStore/tables/Files/movies.csv\")\n",
    "# schemat = moviesDf.schema\n",
    "\n",
    "# wczytujemy plik\n",
    "moviesP = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .schema(schemat) \\\n",
    "    .load(fileFullPath)\n",
    "\n",
    "display(moviesP.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b520b235-9454-4ed0-bccc-2faa5b4401f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Read Modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c83120ab-4321-49df-afef-c37ca543b806",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Wczytujemy plik movies.csv i określamy schemat, gdzie kolumna rok ma mieć typ int. Zmieniamy pierwszy wiersz naszych danych, tak aby otrzymał błędny typ danych. Rok ma typ integer a w pliku new_movies dla filmu Miss Jerry daliśmy wartosc \"??\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d470e33-354f-4d4f-997a-3fb2bfa6070d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>tytuł</th><th>oryginalny tytuł</th><th>rok</th></tr></thead><tbody><tr><td>tt0000009</td><td>Miss Jerry</td><td>Miss Jerry</td><td>??</td></tr><tr><td>tt0000574</td><td>The Story of the Kelly Gang</td><td>The Story of the Kelly Gang</td><td>1906</td></tr><tr><td>tt0001892</td><td>Den sorte drøm</td><td>Den sorte drøm</td><td>1911</td></tr><tr><td>tt0002101</td><td>Cleopatra</td><td>Cleopatra</td><td>1912</td></tr><tr><td>tt0002130</td><td>L'Inferno</td><td>L'Inferno</td><td>1911</td></tr><tr><td>tt0002199</td><td>From the Manger to the Cross or, Jesus of Nazareth</td><td>From the Manger to the Cross or, Jesus of Nazareth</td><td>1912</td></tr><tr><td>tt0002423</td><td>Madame DuBarry</td><td>Madame DuBarry</td><td>1919</td></tr><tr><td>tt0002445</td><td>Quo Vadis?</td><td>Quo Vadis?</td><td>1913</td></tr><tr><td>tt0002452</td><td>Independenta Romaniei</td><td>Independenta Romaniei</td><td>1912</td></tr><tr><td>tt0002461</td><td>Richard III</td><td>Richard III</td><td>1912</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "tt0000009",
         "Miss Jerry",
         "Miss Jerry",
         "??"
        ],
        [
         "tt0000574",
         "The Story of the Kelly Gang",
         "The Story of the Kelly Gang",
         "1906"
        ],
        [
         "tt0001892",
         "Den sorte drøm",
         "Den sorte drøm",
         "1911"
        ],
        [
         "tt0002101",
         "Cleopatra",
         "Cleopatra",
         "1912"
        ],
        [
         "tt0002130",
         "L'Inferno",
         "L'Inferno",
         "1911"
        ],
        [
         "tt0002199",
         "From the Manger to the Cross or, Jesus of Nazareth",
         "From the Manger to the Cross or, Jesus of Nazareth",
         "1912"
        ],
        [
         "tt0002423",
         "Madame DuBarry",
         "Madame DuBarry",
         "1919"
        ],
        [
         "tt0002445",
         "Quo Vadis?",
         "Quo Vadis?",
         "1913"
        ],
        [
         "tt0002452",
         "Independenta Romaniei",
         "Independenta Romaniei",
         "1912"
        ],
        [
         "tt0002461",
         "Richard III",
         "Richard III",
         "1912"
        ]
       ],
       "datasetInfos": [
        {
         "name": "movies",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "tytuł",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "oryginalny tytuł",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "rok",
            "nullable": true,
            "type": "integer"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "org.apache.spark.sql.DataFrame"
        },
        {
         "name": "new_movies",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "tytuł",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "oryginalny tytuł",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "rok",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "org.apache.spark.sql.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tytuł",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "oryginalny tytuł",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "rok",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val schemat = StructType(Array( \n",
    "    StructField(\"id\", StringType, false),\n",
    "    StructField(\"tytuł\", StringType, false),\n",
    "    StructField(\"oryginalny tytuł\", StringType, false),\n",
    "    StructField(\"rok\", IntegerType, false)))        // INTEGERTYPE\n",
    "\n",
    "val movies = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .schema(schemat)\n",
    "  .load(\"dbfs:/FileStore/tables/Files/movies.csv\")\n",
    "\n",
    "movies.printSchema()\n",
    "display(movies)\n",
    "\n",
    "val new_movies = movies.withColumn(\n",
    "  \"rok\", \n",
    "  when(col(\"rok\") === 1894, lit(\"??\")).otherwise(col(\"rok\")) // Miss Jerry\n",
    ")\n",
    "display(new_movies.limit(10))\n",
    "\n",
    "// ZAPISUJEMY ZMIENIONNY PLIK\n",
    "// new_movies.write.format(\"csv\")\n",
    "//   .option(\"header\", \"true\")\n",
    "//   .save(\"dbfs:/FileStore/tables/Files/new_movies.csv\")\n",
    "\n",
    "// DO USUWANIA\n",
    "// dbutils.fs.rm(\"dbfs:/FileStore/tables/Files/new_movies.csv\", true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8630412a-9323-475b-bea5-2188f4f0e912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Bad Records Path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dd25957-5759-4dab-af4a-f9ce216f699a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>tytuł</th><th>oryginalny tytuł</th><th>rok</th></tr></thead><tbody><tr><td>tt0000574</td><td>The Story of the Kelly Gang</td><td>The Story of the Kelly Gang</td><td>1906</td></tr><tr><td>tt0001892</td><td>Den sorte drøm</td><td>Den sorte drøm</td><td>1911</td></tr><tr><td>tt0002101</td><td>Cleopatra</td><td>Cleopatra</td><td>1912</td></tr><tr><td>tt0002130</td><td>L'Inferno</td><td>L'Inferno</td><td>1911</td></tr><tr><td>tt0002199</td><td>From the Manger to the Cross or, Jesus of Nazareth</td><td>From the Manger to the Cross or, Jesus of Nazareth</td><td>1912</td></tr><tr><td>tt0002423</td><td>Madame DuBarry</td><td>Madame DuBarry</td><td>1919</td></tr><tr><td>tt0002445</td><td>Quo Vadis?</td><td>Quo Vadis?</td><td>1913</td></tr><tr><td>tt0002452</td><td>Independenta Romaniei</td><td>Independenta Romaniei</td><td>1912</td></tr><tr><td>tt0002461</td><td>Richard III</td><td>Richard III</td><td>1912</td></tr><tr><td>tt0002646</td><td>Atlantis</td><td>Atlantis</td><td>1913</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "tt0000574",
         "The Story of the Kelly Gang",
         "The Story of the Kelly Gang",
         1906
        ],
        [
         "tt0001892",
         "Den sorte drøm",
         "Den sorte drøm",
         1911
        ],
        [
         "tt0002101",
         "Cleopatra",
         "Cleopatra",
         1912
        ],
        [
         "tt0002130",
         "L'Inferno",
         "L'Inferno",
         1911
        ],
        [
         "tt0002199",
         "From the Manger to the Cross or, Jesus of Nazareth",
         "From the Manger to the Cross or, Jesus of Nazareth",
         1912
        ],
        [
         "tt0002423",
         "Madame DuBarry",
         "Madame DuBarry",
         1919
        ],
        [
         "tt0002445",
         "Quo Vadis?",
         "Quo Vadis?",
         1913
        ],
        [
         "tt0002452",
         "Independenta Romaniei",
         "Independenta Romaniei",
         1912
        ],
        [
         "tt0002461",
         "Richard III",
         "Richard III",
         1912
        ],
        [
         "tt0002646",
         "Atlantis",
         "Atlantis",
         1913
        ]
       ],
       "datasetInfos": [
        {
         "name": "filewithschema1",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "tytuł",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "oryginalny tytuł",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "rok",
            "nullable": true,
            "type": "integer"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "org.apache.spark.sql.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tytuł",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "oryginalny tytuł",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "rok",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "\n",
    "// tworzymy folder\n",
    "// dbutils.fs.mkdirs(\"/mnt/source/badrecords\")\n",
    "\n",
    "val filewithschema1 = spark.read.format(\"csv\")    \n",
    "  .option(\"header\", \"true\")\n",
    "  .schema(schemat)\n",
    "  .option(\"badRecordsPath\", \"/mnt/source/badrecords\")\n",
    "  .load(\"dbfs:/FileStore/tables/Files/new_movies.csv\")\n",
    "\n",
    "display(dbutils.fs.ls(\"/mnt/source/badrecords\"))\n",
    "display(filewithschema1.limit(10)) // nie ma błędnego rekordu "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23195eb5-adfa-421f-9bc6-bf630ccea84d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "PERMISSIVE mode - błędne dane zastępuje przez null<br>\n",
    "W pierwszym wierszu kolumna rok ma zmienioną wartość na null  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aee3fe50-e435-4cf2-b6f5-6cd72fa6fa37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>tytuł</th><th>oryginalny tytuł</th><th>rok</th></tr></thead><tbody><tr><td>tt0000009</td><td>Miss Jerry</td><td>Miss Jerry</td><td>null</td></tr><tr><td>tt0000574</td><td>The Story of the Kelly Gang</td><td>The Story of the Kelly Gang</td><td>1906</td></tr><tr><td>tt0001892</td><td>Den sorte drøm</td><td>Den sorte drøm</td><td>1911</td></tr><tr><td>tt0002101</td><td>Cleopatra</td><td>Cleopatra</td><td>1912</td></tr><tr><td>tt0002130</td><td>L'Inferno</td><td>L'Inferno</td><td>1911</td></tr><tr><td>tt0002199</td><td>From the Manger to the Cross or, Jesus of Nazareth</td><td>From the Manger to the Cross or, Jesus of Nazareth</td><td>1912</td></tr><tr><td>tt0002423</td><td>Madame DuBarry</td><td>Madame DuBarry</td><td>1919</td></tr><tr><td>tt0002445</td><td>Quo Vadis?</td><td>Quo Vadis?</td><td>1913</td></tr><tr><td>tt0002452</td><td>Independenta Romaniei</td><td>Independenta Romaniei</td><td>1912</td></tr><tr><td>tt0002461</td><td>Richard III</td><td>Richard III</td><td>1912</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "tt0000009",
         "Miss Jerry",
         "Miss Jerry",
         null
        ],
        [
         "tt0000574",
         "The Story of the Kelly Gang",
         "The Story of the Kelly Gang",
         1906
        ],
        [
         "tt0001892",
         "Den sorte drøm",
         "Den sorte drøm",
         1911
        ],
        [
         "tt0002101",
         "Cleopatra",
         "Cleopatra",
         1912
        ],
        [
         "tt0002130",
         "L'Inferno",
         "L'Inferno",
         1911
        ],
        [
         "tt0002199",
         "From the Manger to the Cross or, Jesus of Nazareth",
         "From the Manger to the Cross or, Jesus of Nazareth",
         1912
        ],
        [
         "tt0002423",
         "Madame DuBarry",
         "Madame DuBarry",
         1919
        ],
        [
         "tt0002445",
         "Quo Vadis?",
         "Quo Vadis?",
         1913
        ],
        [
         "tt0002452",
         "Independenta Romaniei",
         "Independenta Romaniei",
         1912
        ],
        [
         "tt0002461",
         "Richard III",
         "Richard III",
         1912
        ]
       ],
       "datasetInfos": [
        {
         "name": "filewithschema2",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "tytuł",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "oryginalny tytuł",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "rok",
            "nullable": true,
            "type": "integer"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "org.apache.spark.sql.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tytuł",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "oryginalny tytuł",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "rok",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "val filewithschema2 = spark.read.format(\"csv\")    \n",
    "  .option(\"header\", \"true\")\n",
    "  .schema(schemat)  \n",
    "  .option(\"mode\", \"PERMISSIVE\")\n",
    "  .load(\"dbfs:/FileStore/tables/Files/new_movies.csv\")\n",
    "\n",
    "display(filewithschema2.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72298489-4670-4862-814a-f00fbed92cd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DROPMALFORMED mode - usuwa błędne wiersze\n",
    "Znika pierwszy wiersz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69fe45d6-9eb0-4380-b5e4-6d1148c8695e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>tytuł</th><th>oryginalny tytuł</th><th>rok</th></tr></thead><tbody><tr><td>tt0000574</td><td>The Story of the Kelly Gang</td><td>The Story of the Kelly Gang</td><td>1906</td></tr><tr><td>tt0001892</td><td>Den sorte drøm</td><td>Den sorte drøm</td><td>1911</td></tr><tr><td>tt0002101</td><td>Cleopatra</td><td>Cleopatra</td><td>1912</td></tr><tr><td>tt0002130</td><td>L'Inferno</td><td>L'Inferno</td><td>1911</td></tr><tr><td>tt0002199</td><td>From the Manger to the Cross or, Jesus of Nazareth</td><td>From the Manger to the Cross or, Jesus of Nazareth</td><td>1912</td></tr><tr><td>tt0002423</td><td>Madame DuBarry</td><td>Madame DuBarry</td><td>1919</td></tr><tr><td>tt0002445</td><td>Quo Vadis?</td><td>Quo Vadis?</td><td>1913</td></tr><tr><td>tt0002452</td><td>Independenta Romaniei</td><td>Independenta Romaniei</td><td>1912</td></tr><tr><td>tt0002461</td><td>Richard III</td><td>Richard III</td><td>1912</td></tr><tr><td>tt0002646</td><td>Atlantis</td><td>Atlantis</td><td>1913</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "tt0000574",
         "The Story of the Kelly Gang",
         "The Story of the Kelly Gang",
         1906
        ],
        [
         "tt0001892",
         "Den sorte drøm",
         "Den sorte drøm",
         1911
        ],
        [
         "tt0002101",
         "Cleopatra",
         "Cleopatra",
         1912
        ],
        [
         "tt0002130",
         "L'Inferno",
         "L'Inferno",
         1911
        ],
        [
         "tt0002199",
         "From the Manger to the Cross or, Jesus of Nazareth",
         "From the Manger to the Cross or, Jesus of Nazareth",
         1912
        ],
        [
         "tt0002423",
         "Madame DuBarry",
         "Madame DuBarry",
         1919
        ],
        [
         "tt0002445",
         "Quo Vadis?",
         "Quo Vadis?",
         1913
        ],
        [
         "tt0002452",
         "Independenta Romaniei",
         "Independenta Romaniei",
         1912
        ],
        [
         "tt0002461",
         "Richard III",
         "Richard III",
         1912
        ],
        [
         "tt0002646",
         "Atlantis",
         "Atlantis",
         1913
        ]
       ],
       "datasetInfos": [
        {
         "name": "filewithschema3",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "tytuł",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "oryginalny tytuł",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "rok",
            "nullable": true,
            "type": "integer"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "org.apache.spark.sql.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tytuł",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "oryginalny tytuł",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "rok",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "val filewithschema3 = spark.read.format(\"csv\")    \n",
    "  .option(\"header\", \"true\")\n",
    "  .schema(schemat) \n",
    "  .option(\"mode\", \"DROPMALFORMED\")\n",
    "  .load(\"dbfs:/FileStore/tables/Files/new_movies.csv\")\n",
    "\n",
    "display(filewithschema3.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a88238f-35ab-4586-a017-9d52234f38fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "FAILFAST mode - proces odczytu zostaje zatrzymany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a261541-b713-4cd9-848d-9443f450b40d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 308.0 failed 1 times, most recent failure: Lost task 0.0 in stage 308.0 (TID 791) (ip-10-172-222-69.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/tables/Files/new_movies.csv/part-00000-tid-683814727228870344-ef86fe02-f702-457d-af3a-9d833fc2547a-454-1-c000.csv.\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:704)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:673)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:796)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:496)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1936)\n",
       "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:103)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n",
       "\t... 29 more\n",
       "Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"??\"\n",
       "Caused by: java.lang.NumberFormatException: For input string: \"??\"\n",
       "\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n",
       "\tat java.lang.Integer.parseInt(Integer.java:580)\n",
       "\tat java.lang.Integer.parseInt(Integer.java:615)\n",
       "\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\n",
       "\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\n",
       "\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:202)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:202)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:305)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:202)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.convert(UnivocityParser.scala:360)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:321)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:499)\n",
       "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:796)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:496)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3440)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3362)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3351)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3351)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1460)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1460)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1460)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3651)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3589)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3577)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1209)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1197)\n",
       "\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2758)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:349)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:293)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:377)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:128)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:135)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:122)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:110)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:92)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:537)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:529)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:549)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:402)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:395)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:289)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:506)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:503)\n",
       "\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3458)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$collectResult$1(Dataset.scala:3449)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4373)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:841)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4371)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n",
       "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4371)\n",
       "\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3448)\n",
       "\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:267)\n",
       "\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:101)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$getResultBufferInternal$3(ScalaDriverLocal.scala:347)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$getResultBufferInternal$1(ScalaDriverLocal.scala:327)\n",
       "\tat scala.Option.map(Option.scala:230)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.getResultBufferInternal(ScalaDriverLocal.scala:291)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.getResultBuffer(DriverLocal.scala:1059)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:269)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$27(DriverLocal.scala:929)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:125)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:920)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:77)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:77)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:889)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:719)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:711)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:739)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:628)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:663)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:499)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:438)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:262)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/tables/Files/new_movies.csv/part-00000-tid-683814727228870344-ef86fe02-f702-457d-af3a-9d833fc2547a-454-1-c000.csv.\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:704)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:673)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:796)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:496)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\t... 1 more\n",
       "Caused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1936)\n",
       "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:103)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n",
       "\t... 29 more\n",
       "Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"??\"\n",
       "Caused by: java.lang.NumberFormatException: For input string: \"??\"\n",
       "\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n",
       "\tat java.lang.Integer.parseInt(Integer.java:580)\n",
       "\tat java.lang.Integer.parseInt(Integer.java:615)\n",
       "\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\n",
       "\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\n",
       "\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:202)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:202)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:305)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:202)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.convert(UnivocityParser.scala:360)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:321)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:499)\n",
       "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:796)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:496)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 308.0 failed 1 times, most recent failure: Lost task 0.0 in stage 308.0 (TID 791) (ip-10-172-222-69.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/tables/Files/new_movies.csv/part-00000-tid-683814727228870344-ef86fe02-f702-457d-af3a-9d833fc2547a-454-1-c000.csv.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:704)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:673)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:796)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:496)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1936)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:103)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n\t... 29 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"??\"\nCaused by: java.lang.NumberFormatException: For input string: \"??\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Integer.parseInt(Integer.java:580)\n\tat java.lang.Integer.parseInt(Integer.java:615)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:305)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.convert(UnivocityParser.scala:360)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:321)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:499)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:796)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:496)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3440)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3362)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1460)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1460)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1460)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3651)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3589)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3577)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1209)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1197)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2758)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:349)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:293)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:377)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:128)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:135)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:122)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:110)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:92)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:537)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:529)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:549)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:402)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:395)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:289)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:506)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:503)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3458)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectResult$1(Dataset.scala:3449)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4373)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:841)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4371)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4371)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3448)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:267)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:101)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$getResultBufferInternal$3(ScalaDriverLocal.scala:347)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$getResultBufferInternal$1(ScalaDriverLocal.scala:327)\n\tat scala.Option.map(Option.scala:230)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.getResultBufferInternal(ScalaDriverLocal.scala:291)\n\tat com.databricks.backend.daemon.driver.DriverLocal.getResultBuffer(DriverLocal.scala:1059)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$27(DriverLocal.scala:929)\n\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:125)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$24(DriverLocal.scala:920)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:77)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:77)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:889)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:719)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:711)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:739)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:628)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:663)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:499)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:438)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:262)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:/FileStore/tables/Files/new_movies.csv/part-00000-tid-683814727228870344-ef86fe02-f702-457d-af3a-9d833fc2547a-454-1-c000.csv.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:704)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:673)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:796)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:496)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1936)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:103)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n\t... 29 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"??\"\nCaused by: java.lang.NumberFormatException: For input string: \"??\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Integer.parseInt(Integer.java:580)\n\tat java.lang.Integer.parseInt(Integer.java:615)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:305)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.convert(UnivocityParser.scala:360)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:321)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:499)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:796)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:496)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "FileReadException: Error while reading file dbfs:/FileStore/tables/Files/new_movies.csv/part-00000-tid-683814727228870344-ef86fe02-f702-457d-af3a-9d833fc2547a-454-1-c000.csv.\nCaused by: SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\nCaused by: BadRecordException: java.lang.NumberFormatException: For input string: \"??\"\nCaused by: NumberFormatException: For input string: \"??\"",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "val filewithschema4 = spark.read.format(\"csv\")    \n",
    "  .option(\"header\", \"true\")\n",
    "  .schema(schemat) \n",
    "  .option(\"mode\", \"FAILFAST\")\n",
    "  .load(\"dbfs:/FileStore/tables/Files/new_movies.csv\")\n",
    "\n",
    "display(filewithschema4.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89f8f30f-2c26-43e3-aa53-693cb432e18c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data Frame Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a568fab2-53bf-4835-b15a-744cac20d9a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>oryginalny tytuł</th><th>rok</th><th>tytuł</th></tr></thead><tbody><tr><td>tt0000009</td><td>Miss Jerry</td><td>1894</td><td>Miss Jerry</td></tr><tr><td>tt0000574</td><td>The Story of the Kelly Gang</td><td>1906</td><td>The Story of the Kelly Gang</td></tr><tr><td>tt0001892</td><td>Den sorte drøm</td><td>1911</td><td>Den sorte drøm</td></tr><tr><td>tt0002101</td><td>Cleopatra</td><td>1912</td><td>Cleopatra</td></tr><tr><td>tt0002130</td><td>L'Inferno</td><td>1911</td><td>L'Inferno</td></tr><tr><td>tt0002199</td><td>From the Manger to the Cross or, Jesus of Nazareth</td><td>1912</td><td>From the Manger to the Cross or, Jesus of Nazareth</td></tr><tr><td>tt0002423</td><td>Madame DuBarry</td><td>1919</td><td>Madame DuBarry</td></tr><tr><td>tt0002445</td><td>Quo Vadis?</td><td>1913</td><td>Quo Vadis?</td></tr><tr><td>tt0002452</td><td>Independenta Romaniei</td><td>1912</td><td>Independenta Romaniei</td></tr><tr><td>tt0002461</td><td>Richard III</td><td>1912</td><td>Richard III</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "tt0000009",
         "Miss Jerry",
         "1894",
         "Miss Jerry"
        ],
        [
         "tt0000574",
         "The Story of the Kelly Gang",
         "1906",
         "The Story of the Kelly Gang"
        ],
        [
         "tt0001892",
         "Den sorte drøm",
         "1911",
         "Den sorte drøm"
        ],
        [
         "tt0002101",
         "Cleopatra",
         "1912",
         "Cleopatra"
        ],
        [
         "tt0002130",
         "L'Inferno",
         "1911",
         "L'Inferno"
        ],
        [
         "tt0002199",
         "From the Manger to the Cross or, Jesus of Nazareth",
         "1912",
         "From the Manger to the Cross or, Jesus of Nazareth"
        ],
        [
         "tt0002423",
         "Madame DuBarry",
         "1919",
         "Madame DuBarry"
        ],
        [
         "tt0002445",
         "Quo Vadis?",
         "1913",
         "Quo Vadis?"
        ],
        [
         "tt0002452",
         "Independenta Romaniei",
         "1912",
         "Independenta Romaniei"
        ],
        [
         "tt0002461",
         "Richard III",
         "1912",
         "Richard III"
        ]
       ],
       "datasetInfos": [
        {
         "name": "json",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "oryginalny tytuł",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "rok",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "tytuł",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "org.apache.spark.sql.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "oryginalny tytuł",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "rok",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tytuł",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "moviesS.write.format(\"json\").save(\"dbfs:/FileStore/tables/Files/movies.json\")\n",
    "\n",
    "val json = spark.read.format(\"json\")\n",
    "    .load(\"dbfs:/FileStore/tables/Files/movies.json\")\n",
    "\n",
    "display(json.limit(10))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "notatnik 2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
